# Improved Ghibli Style Image Generator
# For Google Colab with Gradio interface

!pip install -q diffusers transformers accelerate gradio torch

import torch
import gradio as gr
from PIL import Image
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
from diffusers.utils import load_image
import cv2
import numpy as np
from io import BytesIO

# Check if GPU is available
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Function to prepare the model
def load_model():
    # Load the Ghibli fine-tuned model
    model_id = "nitrosocke/Ghibli-Diffusion"
    
    # Load ControlNet for better person image processing
    controlnet = ControlNetModel.from_pretrained(
        "lllyasviel/sd-controlnet-canny", 
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )
    
    # Create pipeline with ControlNet for better control over the output
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        model_id,
        controlnet=controlnet,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        safety_checker=None
    )
    
    # Use more efficient scheduler
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    
    # Move to GPU if available
    pipe = pipe.to(device)
    
    # Enable memory optimizations
    if device == "cuda":
        pipe.enable_attention_slicing()
        pipe.enable_xformers_memory_efficient_attention()
    
    return pipe

# Function to create canny edge map from image
def make_canny_condition(image, low_threshold=100, high_threshold=200):
    image_np = np.array(image)
    image_gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)
    edges = cv2.Canny(image_gray, low_threshold, high_threshold)
    edges = edges[:, :, None]
    edges = np.concatenate([edges, edges, edges], axis=2)
    return Image.fromarray(edges)

# Function to generate Ghibli-style image
def generate_ghibli_image(input_image, prompt_text, guidance_scale, steps):
    if input_image is None:
        return None
    
    # Load the model
    pipe = load_model()
    
    # Resize image if too large (to save memory)
    width, height = input_image.size
    max_size = 768
    if width > max_size or height > max_size:
        if width > height:
            new_width = max_size
            new_height = int(max_size * height / width)
        else:
            new_height = max_size
            new_width = int(max_size * width / height)
        input_image = input_image.resize((new_width, new_height))
    
    # Create canny edge condition from input image
    canny_image = make_canny_condition(input_image)
    
    # Build complete prompt
    if not prompt_text:
        prompt_text = "person"
    
    full_prompt = f"{prompt_text}, studio ghibli style, ghibli anime style, hayao miyazaki style, vibrant colors, detailed"
    
    # Generate image
    try:
        with torch.inference_mode():
            result = pipe(
                prompt=full_prompt,
                image=canny_image,
                guidance_scale=guidance_scale,
                num_inference_steps=steps,
                controlnet_conditioning_scale=1.0,
            ).images[0]
        
        # Clean up GPU memory
        del pipe
        if device == "cuda":
            torch.cuda.empty_cache()
            
        return result
    except Exception as e:
        print(f"Error generating image: {e}")
        # Clean up GPU memory
        del pipe
        if device == "cuda":
            torch.cuda.empty_cache()
        return None

# Alternative method: Direct text-to-image without control net
def generate_text_to_image(prompt_text, guidance_scale, steps):
    # Load the model but without ControlNet for direct text-to-image
    model_id = "nitrosocke/Ghibli-Diffusion"
    from diffusers import DiffusionPipeline
    
    pipe = DiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        safety_checker=None
    )
    
    # Use more efficient scheduler
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
    
    # Move to GPU if available
    pipe = pipe.to(device)
    
    # Enable memory optimizations
    if device == "cuda":
        pipe.enable_attention_slicing()
        try:
            pipe.enable_xformers_memory_efficient_attention()
        except:
            pass
    
    # Build complete prompt
    if not prompt_text:
        prompt_text = "person"
    
    full_prompt = f"{prompt_text}, studio ghibli style, ghibli anime style, hayao miyazaki style, vibrant colors, detailed"
    
    # Generate image
    try:
        with torch.inference_mode():
            result = pipe(
                prompt=full_prompt,
                guidance_scale=guidance_scale,
                num_inference_steps=steps,
            ).images[0]
        
        # Clean up GPU memory
        del pipe
        if device == "cuda":
            torch.cuda.empty_cache()
            
        return result
    except Exception as e:
        print(f"Error generating image: {e}")
        # Clean up GPU memory
        del pipe
        if device == "cuda":
            torch.cuda.empty_cache()
        return None

# Create Gradio interface
def main():
    with gr.Blocks() as demo:
        gr.Markdown("# Studio Ghibli Style Image Generator")
        gr.Markdown("Create Ghibli-style images either from an uploaded photo or from text description")
        
        with gr.Tabs():
            # Tab 1: Image-to-Image with ControlNet
            with gr.TabItem("Transform Photo (Image-to-Image)"):
                with gr.Row():
                    with gr.Column():
                        input_image = gr.Image(type="pil", label="Upload Image")
                        prompt_text = gr.Textbox(label="Description (optional)", placeholder="person in a park")
                        guidance_scale = gr.Slider(2, 15, 7.5, step=0.5, label="Guidance Scale")
                        steps = gr.Slider(20, 50, 30, step=5, label="Steps")
                        submit_btn = gr.Button("Generate Ghibli Style Image")
                    
                    with gr.Column():
                        output_image = gr.Image(label="Ghibli Style Result")
                
                # Connect the function
                submit_btn.click(
                    fn=generate_ghibli_image,
                    inputs=[input_image, prompt_text, guidance_scale, steps],
                    outputs=output_image
                )
            
            # Tab 2: Text-to-Image
            with gr.TabItem("Generate from Description (Text-to-Image)"):
                with gr.Row():
                    with gr.Column():
                        txt2img_prompt = gr.Textbox(label="Description", placeholder="A person standing on a hill overlooking a beautiful landscape")
                        txt2img_guidance_scale = gr.Slider(2, 15, 7.5, step=0.5, label="Guidance Scale")
                        txt2img_steps = gr.Slider(20, 50, 30, step=5, label="Steps")
                        txt2img_submit_btn = gr.Button("Generate Ghibli Style Image")
                    
                    with gr.Column():
                        txt2img_output = gr.Image(label="Ghibli Style Result")
                
                # Connect the function
                txt2img_submit_btn.click(
                    fn=generate_text_to_image,
                    inputs=[txt2img_prompt, txt2img_guidance_scale, txt2img_steps],
                    outputs=txt2img_output
                )
        
        gr.Markdown("""
        ## How to use:
        
        ### Option 1: Transform a Photo
        1. Upload an image of a person (or any subject)
        2. Add an optional description to guide the style
        3. Adjust parameters as needed
        4. Click "Generate Ghibli Style Image"
        
        ### Option 2: Generate from Description
        1. Enter a detailed description of what you want to create
        2. Adjust parameters as needed
        3. Click "Generate Ghibli Style Image"
        
        ## Tips for Best Results:
        - For people, try prompts like "young woman with long hair" or "man in traditional clothes"
        - Adding location details helps: "person in a meadow" or "character in a magical forest"
        - Increase Guidance Scale for stronger Ghibli style
        - More steps = better quality but slower generation
        
        ## Note:
        - First generation may take longer as the model loads
        - If you encounter memory issues, try reducing steps or restarting the runtime
        """)
    
    # Launch the interface
    demo.launch(debug=True)

# Run the app
if __name__ == "__main__":
    main()
